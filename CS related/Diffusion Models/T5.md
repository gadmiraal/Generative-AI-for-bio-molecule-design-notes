## Architecture

## Timestep Extension
We want our T5 architecture to be able to be used in our diffusion context. This is why that we need to add conditioning information, our timestep, to the input of this model. This conditioning information is only used in the decoder part of our T5 architecture.
### FiLM
FiLM, which stands for Feature-wise Linear Modulation, is a neural network conditioning method that adjusts intermediate feature maps using learned affine transformations. The key idea is to modulate the features of a neural network by applying linear transformations that are conditioned on some input, such as a task descriptor, an image, or a timestep.

The core concept of FiLM is to apply a feature-wise affine transformation to the feature maps of a neural network. For each feature map FFF in the network, FiLM can apply a scaling factor $\gamma$$ and/or a bias $\beta$
$$
\text{FiLM}(F;\gamma, \beta)=\gamma \odot F + \beta
$$
Feature-wise modulation often strike a happy compromise between effectiveness and efficiency: the number of scaling and/or shifting coefficients to predict scales linearly with the number of features in the network. Also, in practice, feature-wise transformations (often compounded across multiple layers) frequently have enough capacity to model complex phenomenon in various settings.

### FiLMBlock
This FiLM block adds a scaling and a shifting to its input. It is used at the beginning of each T5 Decoder layer. This ensures that 
$$
x=x\times\text{embed}(t) + \text{embed}(t)
$$


### TimestepEmbedBlock
Embeds the timestep to the embedding space and this embedding is added to input.
$$
x=x+\text{embed}(t)
$$
This block is used at the beginning of the T5 Decoder Block. This is because this ensures that all  layers are aware of the temporal context right from the start.


