## Architecture
Text-to-Text Transfer Transformer (T5) was created to explore the transfer learning landscape for NLP. It is based on the vanilla transformer encoder-decoder with some slight modifications. This type of transformer is capable of being trained on a variety of tasks with a uniform architecture.

![](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3f25f11e-1daf-4711-940a-6b09a1f62ae7_2298x1474.png)
*Modifications made by T5 to the encoder-decoder transformer architecture*

The changes that set T5 apart from the standard transformer are as follows:
1. [LayerNorm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html) is applied immediately before each attention and feed forward transformation (i.e., outside of the residual path)
2. No additive bias is used for LayerNorm (i.e., see [here](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html); we only use scale and eliminate the additive bias)
3. A simple position embedding scheme is used that adds a scalar to the corresponding [logit](https://stackoverflow.com/questions/41455101/what-is-the-meaning-of-the-word-logits-in-tensorflow) used to compute attention weights
4. Dropout is applied throughout the network (e.g., attention weights, feed forward network, skip connection, etc.)

## Timestep Extension
We want our T5 architecture to be able to be used in our diffusion context. This is why that we need to add conditioning information, our timestep, to the input of this model. This conditioning information is only used in the decoder part of our T5 architecture.
### FiLM
FiLM, which stands for Feature-wise Linear Modulation, is a neural network conditioning method that adjusts intermediate feature maps using learned affine transformations. The key idea is to modulate the features of a neural network by applying linear transformations that are conditioned on some input, such as a task descriptor, an image, or a timestep.

The core concept of FiLM is to apply a feature-wise affine transformation to the feature maps of a neural network. For each feature map $F$ in the network, FiLM can apply a scaling factor $\gamma$ and/or a bias $\beta$
$$
\text{FiLM}(F;\gamma, \beta)=\gamma \odot F + \beta
$$
Performing a linear shifting on the feature input results in <span style="color:#ff0000">REASON</span>. While performing a multiplicative interaction gives us the ability to learn the relationship between input and condition. This is also why dot-product between vectors can be used to see how similar they are.

Feature-wise modulation often strike a happy compromise between effectiveness and efficiency: the number of scaling and/or shifting coefficients to predict scales linearly with the number of features in the network. Also, in practice, feature-wise transformations (often compounded across multiple layers) frequently have enough capacity to model complex phenomenon in various settings.

### FiLMBlock
This FiLM block adds a scaling and a shifting to its input. It is used at the beginning of each T5 Decoder layer. 
$$
x=x\times\text{embed}(t) + \text{embed}(t)
$$
Using both a multiplicative and additive interaction on the input allows for fine-grained, feature-wise modulation throughout the network. The multiplicative component adjusts the scale of features based on their relevance to the timestep, while the additive component shifts the features to refine their representation.

### TimestepEmbedBlock
Embeds the timestep to the embedding space and this embedding is added to input.
$$
x=x+\text{embed}(t)
$$
This block is used at the beginning of the T5 Decoder Block and sets the initial context by smoothly integrating the timestep information with the input features. This ensures that all subsequent layers have a baseline conditioning context to work from.


